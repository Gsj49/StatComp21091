---
title: "homework-21091"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework-21091}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
 
# 21091-2021-09-23
 
## Questions

Exercises 3.4, 3.11, 3.20 (in pages 94-96, Statistical Computating with R)

## Answers

### 1.
First we have that 
$$
\begin{aligned}
F(x) &= \int_{-\infty}^x \frac{x}{\sigma^{2}} e^{-x^{2}/(2 \sigma^{2})}\mathbb{I}_{(0,\infty)}(x)dx\\
&=1-e^{-x^{2} /(2 \sigma^{2})}
\end{aligned}
$$
so that we obtain
$$
F^{-1}(u) = \sqrt{-2\sigma^2\ln(1-u)}.
$$
Now applying inverse transform method, we get the following results with different values of $\sigma^2$,
```{r}
sigma = 0.5
n = 10000
u = runif(n)
x = sqrt(-2*sigma^2*log(1-u))
hist(x, prob = TRUE, main = paste0('sigma=',sigma))
y = seq(0,15, .01)
lines(y, y/sigma^2*exp(-y^2/(2*sigma^2)))
```
```{r}
sigma = 1
n = 10000
u = runif(n)
x = sqrt(-2*sigma^2*log(1-u))
hist(x, prob = TRUE, main = paste0('sigma=',sigma))
y = seq(0,15, .01)
lines(y, y/sigma^2*exp(-y^2/(2*sigma^2)))
```
```{r}
sigma = 2
n = 10000
u = runif(n)
x = sqrt(-2*sigma^2*log(1-u))
hist(x, prob = TRUE, main = paste0('sigma=',sigma))
y = seq(0,15, .01)
lines(y, y/sigma^2*exp(-y^2/(2*sigma^2)))
```
```{r}
sigma = 4
n = 10000
u = runif(n)
x = sqrt(-2*sigma^2*log(1-u))
hist(x, prob = TRUE, main = paste0('sigma=',sigma))
y = seq(0,15, .01)
lines(y, y/sigma^2*exp(-y^2/(2*sigma^2)))
```

### 2.
```{r}
plot_mixture_normal_distribution <- function(p1, n = 10000){
  x1 = rnorm(n, 0, 1)
  x2 = rnorm(n, 3, 1)
  r = rbinom(n, 1, p1)
  x = r*x1 + (1-r)*x2
  hist(x,breaks = 30)
}
for (i in seq(0.01,0.9,0.1)) {
  plot_mixture_normal_distribution(i)
}

```
 
The conclusion is, the closer $p_1$ approaches 0.5, the more significant appearance of bimodal.

### 3.
First we show the theoretical results. We have
$$
\begin{aligned}
E(X(t))& = E(E(X(t)|N(t)))\\
& =E(N(t)E(Y_1))=\lambda tE(Y_1)
\end{aligned}
$$
and according to $E(N(t))=\lambda t, E(N(t)^2)=\lambda^2t^2 -\lambda t$,
$$
\begin{aligned}
Var(X(t))&=E(X(t)^2) - E^2(X(t))\\
&=E(E(X(t)^2|N(t)))-(\lambda t\mu)^2\\
&=E\big(N(t)\cdot E(Y_1^2)+(N(t)^2-N(t))E(Y_1)) - (\lambda t\mu)^2\big)\\
&=\lambda tE(Y_1^2).
\end{aligned}
$$
Then, we give simulations as follows,
```{r}
n=1000
Poisson_Gamma_process <- function(t,lambda,alpha,beta,n=1000){
  nt = rpois(n,lambda*t)
  xt = c()
  for (i in 1:n) {
    xt = append(xt,sum(rgamma(nt[i],alpha,beta)))
  }
  return(xt)
}
```
```{r}
print_result <- function(lambda,alpha,beta,n=1000){
  xt = Poisson_Gamma_process(10,lambda,alpha,beta,1000)
  print(paste0("theoretical mean:",10*lambda*alpha/beta))
  print(paste0("theoretical variance:",10*lambda*alpha*(alpha+1)/beta^2))
  print(paste0("estimated mean:",mean(xt)))
  print(paste0("estimated variance:",var(xt)))
}
```

```{r}
print_result(3,3,3,n=1000)
```
```{r}
print_result(3,4,5,n=1000)
```
```{r}
print_result(2,5,2,n=1000)
```
As is shown above, the estimators are accurate.
 
# 21091-2021-09-30

## Questions

Exercises 5.4, 5.9, 5.13, and 5.14 (pages 149-151, Statistical Computating with R).\

## Answers

### 5.4
Since we have
$$
f(x:\alpha,\beta)=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\mathbb{I}_{[0,1]}(x).
$$
For $x\in [0,1] $, we have
$$
\begin{aligned}
F(x:\alpha,\beta) &= \int_0^x \frac{1}{B(\alpha,\beta)}t^{\alpha-1}(1-t)^{\beta-1}dt\\
& = \frac{x}{B(\alpha,\beta)}\int_0^x \frac{1}{x}\cdot t^{\alpha-1}(1-t)^{\beta-1}dt\\
& = \frac{x}{B(\alpha,\beta)}\cdot E_Y\big[Y^{\alpha-1}(1-Y)^{\beta-1}\big]
\end{aligned}
$$
where $Y\sim U(0,x)$. Define the function as follows:(with B(3,3)=30)
```{r}
f1 <- function(x,n){
  u = runif(n,0,x)
  v = u^(2)*(1-u)^(2)
  return(x*mean(v)*30)
}
```
Then we have
```{r}
res = c()
for (i in 0.1*c(1:9)) {
  res <- append(res, f1(i,10000))
}
print(res)
```
The true value of cdf is
```{r}
pbeta(0.1*c(1:9),3,3)
```
### 5.9
First we have that 
$$
\begin{aligned}
F(x) &= \int_{-\infty}^x \frac{x}{\sigma^{2}} e^{-x^{2}/(2 \sigma^{2})}\mathbb{I}_{(0,\infty)}(x)dx\\
&=1-e^{-x^{2} /(2 \sigma^{2})}
\end{aligned}
$$
so that we obtain
$$
F^{-1}(u) = \sqrt{-2\sigma^2\ln(1-u)}.
$$
To apply antithetic method we use $1-u$ in exchange of $u$ to generate another group of samples
```{r}
Rayleigh_no_antithetic <- function(n,sigma){
  u = runif(n,0,1)
  x = sqrt(-2*sigma^2*log(1-u))
  return(x)
}
Rayleigh_with_antithetic <- function(n,sigma){
  u = runif(n,0,1)
  x1 = sqrt(-2*sigma^2*log(1-u))
  x2 = sqrt(-2*sigma^2*log(u))
  return((x1+x2)/2)
}
var_no_antithetic = var(Rayleigh_no_antithetic(10000,2)) 
var_with_antithetic = var(Rayleigh_with_antithetic(10000,2))
print(var_no_antithetic)
print(var_with_antithetic)
print(var_with_antithetic/var_no_antithetic)
```
Thus we have variance reduction equals to 1-0.02698519

### 5.13

First we have
$$
g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1
$$
We choose $X_1\sim N(0,1)$ with density $f_1(x)$, such that
$$
\begin{aligned}
\int_1^\infty g(x)dx &= \int_1^\infty x^2\cdot f_1(x)dx\\
&=E[X_1^2\mathbb{I}_{[1,\infty]}(X_1)]
\end{aligned}
$$
```{r}
x1 = rnorm(100000)
x1[x1<1] = 0
I1 = mean(x1^2)
print(I1)
print(var(x1^2)/100000)
```
estimator1 and its variance are shown above respectively.
 
Secondly we choose $X_2\sim Exp(1)+1$ with density $f_2(x)=e^{-x+1}\mathbb{I}_{[1,\infty]}$, such that
$$
\begin{aligned}
\int_1^\infty g(x)dx& =\int_{1}^\infty \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}dx\\
& = \int_{1}^\infty \frac{x^{2}}{\sqrt{2\pi}} e^{-x^{2}/2+x-1}f_2(x)dx\\
& = \frac{1}{\sqrt{2\pi}}E[X_2^2e^{-X_2^{2}/2(X_2-1)}]
\end{aligned}
$$
```{r}
x2 = rexp(100000,1) + 1
I2 = mean(x2^2*exp(-x2^2/2+x2-1)/sqrt(2*pi))
print(I2)
print(var(x2^2)/100000)
```
estimator2 and its variance are shown above respectively.

### 5.14
As is shown in 5.13, the estimated value is 0.4005021 by importance sampling.
 
# 21091-2021-10-14
 
## Questions
Exercises 6.5 and 6.A (page 180-181, Statistical Computating with R).

## Answers

### 6.5
Since $n=20$, t-statistic: $T=\sqrt{n}(\bar{X}-\mu)/s \sim t_{19}$, we have $(1-\alpha)$ confidence interval:$\big[\bar{X}-s\cdot t_{0.975}/\sqrt{n},\bar{X}+s\cdot t_{0.975}/\sqrt{n}\big]$. When $X\sim \chi^2(2)$, $EX=2$, we estimate the coverage probability as follows,
```{r}
set.seed(123)
n = 20
count = 0
m =10000
for(i in 1:m){
  x = rchisq(n,2)
  lower = mean(x)-qt(0.975,n-1) *sd(x)/sqrt(n)
  upper = mean(x)+qt(0.975,n-1) *sd(x)/sqrt(n)
  if(upper>2 && lower<2)
    count = count + 1
}
# cover probability
print(count/m)
```

As for CI for variance, we have $varX = 4$ and CI:$\big[0,(n-1)s^{2}/\chi_{\alpha}^{2}\big]$, such that
```{r}
set.seed(12)
n = 20
count2 = 0
m =10000
for(i in 1:m){
  x = rchisq(n,2)
  upp = (n-1)*var(x)/qchisq(0.05,n-1)
  if(4< upp)
    count2 = count2 +1
}
# cover probability
print(count2/m)
```
As shown above, 0.928>0.8609, which proves the better robustness of t-interval.

### 6.A
Set $n=50,\alpha= 0.05$. We have t-statistics:$T = \sqrt{n}(\bar{X}-\mu)/s$, rejection zone: $\{|T|>t_{0.975}\}$
case1: $X\sim \chi^2(1)$ with $\mu=EX=1$:
```{r}
n = 50
m = 10000
count1 = 0
for(i in 1:m){
  x = rchisq(n,1)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count1 = count1 + 1
}
print(count1/m)
```
case 2: $X\sim U(0，2)$ with $\mu=EX=1$:
```{r}
n = 50
m = 10000
count2 = 0
for(i in 1:m){
  x = runif(n,0,2)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count2 = count2 + 1
}
print(count2/m)
```
case 3: $X\sim Exp(1)$ with $\mu=EX=1$:
```{r}
n = 50
m = 10000
count3 = 0
for(i in 1:m){
  x = rexp(n,1)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count3 = count3 + 1
}
print(count3/m)
```

The epirical type one error of each case respectively equals to 0.0778, 0.0465 and 0.0662, which is a little close to significance level 0.05. Setting $n=100$ and repeat the simulations, we have
```{r}
set.seed(1234)
n = 100
m = 10000
count1 = 0
for(i in 1:m){
  x = rchisq(n,1)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count1 = count1 + 1
}
print(count1/m)
count2 = 0
for(i in 1:m){
  x = runif(n,0,2)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count2 = count2 + 1
}
print(count2/m)
count3 = 0
for(i in 1:m){
  x = rexp(n,1)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count3 = count3 + 1
}
print(count3/m)
```
epirical type one error: 0.0633, 0.0485, 0.0578, closer to 0.05. It's easy to see that, t1e of uniform distribution deviates least and that of chisquare distribution deviates most.

### question3
If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at 0.05 level.
 
(i) What is the corresponding hypothesis test problem?
 
(ii) What test should we use? Z-test, two-sample t-test, paired-t
test or McNemar test? Why?
 
(iii) Please provide the least necessary information for hypothesis testing.
  
Answers:
 
(i) H0: the powers for two methods are identical. H1: the powers for two methods are different.
 
(ii) McNemar test. This test is applied to 2 × 2 contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal, i.e. $p_b=p_c$ in the following table:
 
|                 | Test 2 positive | Test 2 negative | Row total |
|:---------------:|:---------------:|:---------------:|:---------:|
| Test 1 positive |        a        |        b        |   a + b   |
| Test 1 negative |        c        |        d        |   c + d   |
|   Column total  |      a + c      |      b + d      |     N     |
 
which means to test whether $\text{power of test1}=p_a+p_b = p_a+p_b=\text{power of test2}$.

 
(iii) Record the detailed test decision (value of a,b,c,d in the above table) of each experiment of both methods, and follow the McNemar test procedure.
 
# 21091-2021-10-21 

## Questions
Exercises 6.C (pages 182, Statistical Computating with R).

## Answers

### 6.C

#### Example 6.8
```{r}
library(MASS)
```

The function calculating b1d:
```{r}
b1d <- function(x) {
  n = nrow(x)
  d = ncol(x)
  mu_hat = colSums(x)/n # MLE of mu
  #sigma_hat = 0
  #for(i in c(1:n)){
    #sigma_hat = sigma_hat + (x[i,]-mu_hat) %*% t(x[i,]-mu_hat)
  #}
  # sum(apply(x,1,function(y) t(y-mu_hat) %*% inv_sigma_hat %*% (y-mu_hat)))/n
  sigma_hat = cov(x)*(n-1)/n # MLE of cov
  inv_sigma_hat = solve(sigma_hat)
  
  # loop calculation of skewness 
  # for(j1 in c(1:n)){
  #   for(j2 in c(1:n)){
  #     b = b + (t(x[j1,]-mu_hat) %*% inv_sigma_hat %*% (x[j2,]-mu_hat))^3
  #   }
  # }
  # b = b/n^2
  # matrix calculation of skewness
  xc = sweep(x,2,mu_hat)
  b = sum((xc %*% inv_sigma_hat %*% t(xc))^3)/n^2
  return(b)
}

```

Case $d=2$:
```{r}
set.seed(12345)
n <-  c(20,30,50,100)
p.reject <- numeric(length(n))
m <- 1000
d <-  2
upper <- qchisq(0.975, df = d*(d+1)*(d+2)/6)
lower <- qchisq(0.025, df = d*(d+1)*(d+2)/6)
for (i in 1:length(n)) {
  sktests <- numeric(m) 
  for (j in 1:m) {
    x = mvrnorm(n[i],rep(0,d),diag(rep(1,d)))
    b = b1d(x)*n[i]/6
    sktests[j] <- as.integer(b > upper || b < lower ) }
  p.reject[i] <- mean(sktests) 
}
data.frame(n = n, estimate = p.reject)
```

#### Example 6.10
```{r}
alpha <- 0.1
n <- 30
m <- 500
epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
N <- length(epsilon)
power <- numeric(N)
```

```{r}
d = 2 
# two multivariate normal components 
sigma_1 = diag(rep(1,d))
sigma_10 = diag(rep(10,d))
set.seed(4321)
for (j in 1:N) { 
  e <- epsilon[j]
  sktests <- numeric(m)
  for (i in 1:m) {
    # in each replication, count_1 samples ~ sigma_1, count_10 samples ~ sigma_10
    count_1 = rbinom(1, n, 1-e)
    count_10 = n - count_1
    # in case epsilon = 1 or 0; count_1 or count_10 = 0 
    if(count_10==0){
      x = mvrnorm(count_1,rep(0,d),sigma_1)
    } else if(count_1 ==0){
      x =  mvrnorm(count_10,rep(0,d),sigma_10)
    } else{
      x_1 <- mvrnorm(count_1,rep(0,d),sigma_1)
      x_10 <- mvrnorm(count_10,rep(0,d),sigma_10)
      x = rbind(x_1,x_10)
    }
    b = b1d(x)*n/6
    sktests[i] <- as.integer(b > upper|| b<lower) 
    }
  power[j] <- mean(sktests)
}

```



The result is shown below:
```{r}

data.frame(epsilon = epsilon, estimate = power)
```

```{r}
plot(epsilon, power, type = "b", xlab = bquote(epsilon), ylim = c(0,1)) 
abline(h = .1, lty = 3)
se <- sqrt(power * (1-power) / m) #add standard errors 
lines(epsilon, power+se, lty = 3)
lines(epsilon, power-se, lty = 3)
```
 
# 21091-2021-10-28

## Questions
Exercises 7.7, 7.8, 7.9, and 7.B (pages 213, Statistical Computating with R).

## Answers

```{r}
library(bootstrap)
data(scor)
data = scor
```

### 7.7
```{r}
theta <- function(x){
  n = nrow(x)
  Cov = cov(x)*(n-1)/n
  Eigen_values = eigen(Cov)$values
  return(max(Eigen_values)/sum(Eigen_values)) 
}
```

```{r}
set.seed(413321)
theta_hat = theta(data)
cat("sample estimate is:",theta_hat)
```
```{r}
B = 10000
n = nrow(data)
theta_star = numeric(B)
for(i in 1:B){
  index = sample(1:n,n,replace = TRUE)
  x_star = data[index,]
  theta_star[i] = theta(x_star)
}
cat("the bias is",mean(theta_hat-theta_star),"\n")
cat("the sd is",sd(theta_hat-theta_star))
```

### 7.8
```{r}
theta_hat = theta(data)
n = nrow(data)
theta_jacknife = numeric(n)
for(i in 1:n){
  x_jacknife = data[-i,]
  theta_star[i] = theta(x_jacknife)
}
cat("the jacknife estimate of bias is",(n-1)*mean(theta_hat-theta_star),"\n")
cat("the jacknife estimate of sd is",sqrt((n-1)^2*var(theta_hat-theta_star)/n))
```
### 7.9
(1) percentile confidence intervals

```{r}
# we directly use the theta_star generated in ex 7.7
quan_upper = quantile(theta_star,0.975)
quan_lower = quantile(theta_star,0.025)
cat("the interval is [",quan_lower,",",quan_upper,"]")
```
 
(2) BCa confidence intervals
```{r}
boot.BCa <-
  function(x, th0, th, stat, conf = .95) {
    x <- as.matrix(x)
    n <- nrow(x) 
    N <- 1:n
    alpha <- (1 + c(-conf, conf))/2 
    zalpha <- qnorm(alpha)
    z0 <- qnorm(sum(th < th0) / length(th))
    th.jack <- numeric(n)
    for (i in 1:n) {
      J <- N[1:(n-1)]
      th.jack[i] <- stat(x[-i, ]) 
    }
    L <- mean(th.jack) - th.jack
    a <- sum(L^3)/(6 * sum(L^2)^1.5)
    adj.alpha <- pnorm(z0 + (z0+zalpha)/(1-a*(z0+zalpha))) 
    limits <- quantile(th, adj.alpha, type=6) 
    return(list("est"=th0, "BCa"=limits))
}
```

```{r}
boot.BCa(data,theta_hat,theta_star,theta,0.95)
```


### 7.B
```{r}
# first construct estimate of skewness 
skew_statistic <- function(x){
  x_bar <- mean(x)
  m3 <- mean((x-x_bar)^3)
  m2 <- mean((x-x_bar)^2)
  return(m3/m2^(3/2))
}
```



(1) normal distribution

```{r}
n = 20
m = 200
# standard normal bootstrap confidence interval
standard_interval = array(0,c(m,2))
colnames(standard_interval)<- c("lower_bound","upper_bound")

# basic bootstrap confidence interval
basic_interval = array(0,c(m,2))
colnames(basic_interval)<- c("lower_bound","upper_bound")

# percentile confidence interval
percentile_interval = array(0,c(m,2))
colnames(percentile_interval)<- c("lower_bound","upper_bound")

for(i in 1:m){
  x = rnorm(n)
  skewness_hat = skew_statistic(x)
  B = 500
  skewness_star = numeric(B)
  for(j in 1:B){
    index = sample(1:n,n,replace = TRUE)
    x_star = x[index]
    skewness_star[j] = skew_statistic(x_star)
  }
  standard_interval[i,1] = skewness_hat - qnorm(0.975)*sd(skewness_star)
  standard_interval[i,2] = skewness_hat + qnorm(0.975)*sd(skewness_star)
  basic_interval[i,1] = 2*skewness_hat - quantile(skewness_star,0.975)
  basic_interval[i,2] = 2*skewness_hat - quantile(skewness_star,0.025)
  percentile_interval[i,1] = quantile(skewness_star,0.025)
  percentile_interval[i,2] = quantile(skewness_star,0.975)
  
}
```

```{r}
# convergence rate of three kinds of interval
cat("convergence rates for normal distribution shown below:\n")
cat("standard:",1-mean((standard_interval[,1]>0 | standard_interval[,2]<0)),"\n")
cat("basic:",1-mean(basic_interval[,1]>0 | basic_interval[,2]<0),"\n")
cat("percentile:",1-mean(percentile_interval[,1]>0 | percentile_interval[,2]<0))
```

 

(2) Chisquare distribution $\chi^2(5)$ with skewness $\sqrt{(8/5)}$
```{r}
true_skewness = sqrt(8/5)
n = 20
m = 200
# standard normal bootstrap confidence interval
standard_interval = array(0,c(m,2))
colnames(standard_interval)<- c("lower_bound","upper_bound")

# basic bootstrap confidence interval
basic_interval = array(0,c(m,2))
colnames(basic_interval)<- c("lower_bound","upper_bound")

# percentile confidence interval
percentile_interval = array(0,c(m,2))
colnames(percentile_interval)<- c("lower_bound","upper_bound")

for(i in 1:m){
  x = rchisq(n,5)
  skewness_hat = skew_statistic(x)
  B = 500
  skewness_star = numeric(B)
  for(j in 1:B){
    index = sample(1:n,n,replace = TRUE)
    x_star = x[index]
    skewness_star[j] = skew_statistic(x_star)
  }
  standard_interval[i,1] = skewness_hat - qnorm(0.975)*sd(skewness_star)
  standard_interval[i,2] = skewness_hat + qnorm(0.975)*sd(skewness_star)
  basic_interval[i,1] = 2*skewness_hat - quantile(skewness_star,0.975)
  basic_interval[i,2] = 2*skewness_hat - quantile(skewness_star,0.025)
  percentile_interval[i,1] = quantile(skewness_star,0.025)
  percentile_interval[i,2] = quantile(skewness_star,0.975)
  
}
```

```{r}
cat("convergence rates for chisquare distribution shown below:\n")
cat("standard:",1-mean((standard_interval[,1]>true_skewness | standard_interval[,2]<true_skewness)),"\n")
cat("basic:",1-mean(basic_interval[,1]>true_skewness | basic_interval[,2]<true_skewness),"\n")
cat("percentile:",1-mean(percentile_interval[,1]>true_skewness | percentile_interval[,2]<true_skewness))
```


