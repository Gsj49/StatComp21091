---
title: "homework-21091"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework-21091}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
```{r setup}
library(StatComp21091)
```
 
 
# 21091-2021-09-23
 
## Questions

Exercises 3.4, 3.11, 3.20 (in pages 94-96, Statistical Computating with R)

## Answers

### 1.
First we have that 
$$
\begin{aligned}
F(x) &= \int_{-\infty}^x \frac{x}{\sigma^{2}} e^{-x^{2}/(2 \sigma^{2})}\mathbb{I}_{(0,\infty)}(x)dx\\
&=1-e^{-x^{2} /(2 \sigma^{2})}
\end{aligned}
$$
so that we obtain
$$
F^{-1}(u) = \sqrt{-2\sigma^2\ln(1-u)}.
$$
Now applying inverse transform method, we get the following results with different values of $\sigma^2$,
```{r}
sigma = 0.5
n = 10000
u = runif(n)
x = sqrt(-2*sigma^2*log(1-u))
hist(x, prob = TRUE, main = paste0('sigma=',sigma))
y = seq(0,15, .01)
lines(y, y/sigma^2*exp(-y^2/(2*sigma^2)))
```
```{r}
sigma = 1
n = 10000
u = runif(n)
x = sqrt(-2*sigma^2*log(1-u))
hist(x, prob = TRUE, main = paste0('sigma=',sigma))
y = seq(0,15, .01)
lines(y, y/sigma^2*exp(-y^2/(2*sigma^2)))
```
```{r}
sigma = 2
n = 10000
u = runif(n)
x = sqrt(-2*sigma^2*log(1-u))
hist(x, prob = TRUE, main = paste0('sigma=',sigma))
y = seq(0,15, .01)
lines(y, y/sigma^2*exp(-y^2/(2*sigma^2)))
```
```{r}
sigma = 4
n = 10000
u = runif(n)
x = sqrt(-2*sigma^2*log(1-u))
hist(x, prob = TRUE, main = paste0('sigma=',sigma))
y = seq(0,15, .01)
lines(y, y/sigma^2*exp(-y^2/(2*sigma^2)))
```

### 2.
```{r}
plot_mixture_normal_distribution <- function(p1, n = 10000){
  x1 = rnorm(n, 0, 1)
  x2 = rnorm(n, 3, 1)
  r = rbinom(n, 1, p1)
  x = r*x1 + (1-r)*x2
  hist(x,breaks = 30)
}
for (i in seq(0.01,0.9,0.1)) {
  plot_mixture_normal_distribution(i)
}

```
 
The conclusion is, the closer $p_1$ approaches 0.5, the more significant appearance of bimodal.

### 3.
First we show the theoretical results. We have
$$
\begin{aligned}
E(X(t))& = E(E(X(t)|N(t)))\\
& =E(N(t)E(Y_1))=\lambda tE(Y_1)
\end{aligned}
$$
and according to $E(N(t))=\lambda t, E(N(t)^2)=\lambda^2t^2 -\lambda t$,
$$
\begin{aligned}
Var(X(t))&=E(X(t)^2) - E^2(X(t))\\
&=E(E(X(t)^2|N(t)))-(\lambda t\mu)^2\\
&=E\big(N(t)\cdot E(Y_1^2)+(N(t)^2-N(t))E(Y_1)) - (\lambda t\mu)^2\big)\\
&=\lambda tE(Y_1^2).
\end{aligned}
$$
Then, we give simulations as follows,
```{r}
n=1000
Poisson_Gamma_process <- function(t,lambda,alpha,beta,n=1000){
  nt = rpois(n,lambda*t)
  xt = c()
  for (i in 1:n) {
    xt = append(xt,sum(rgamma(nt[i],alpha,beta)))
  }
  return(xt)
}
```
```{r}
print_result <- function(lambda,alpha,beta,n=1000){
  xt = Poisson_Gamma_process(10,lambda,alpha,beta,1000)
  print(paste0("theoretical mean:",10*lambda*alpha/beta))
  print(paste0("theoretical variance:",10*lambda*alpha*(alpha+1)/beta^2))
  print(paste0("estimated mean:",mean(xt)))
  print(paste0("estimated variance:",var(xt)))
}
```

```{r}
print_result(3,3,3,n=1000)
```
```{r}
print_result(3,4,5,n=1000)
```
```{r}
print_result(2,5,2,n=1000)
```
As is shown above, the estimators are accurate.
 
# 21091-2021-09-30

## Questions

Exercises 5.4, 5.9, 5.13, and 5.14 (pages 149-151, Statistical Computating with R).\

## Answers

### 5.4
Since we have
$$
f(x:\alpha,\beta)=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\mathbb{I}_{[0,1]}(x).
$$
For $x\in [0,1] $, we have
$$
\begin{aligned}
F(x:\alpha,\beta) &= \int_0^x \frac{1}{B(\alpha,\beta)}t^{\alpha-1}(1-t)^{\beta-1}dt\\
& = \frac{x}{B(\alpha,\beta)}\int_0^x \frac{1}{x}\cdot t^{\alpha-1}(1-t)^{\beta-1}dt\\
& = \frac{x}{B(\alpha,\beta)}\cdot E_Y\big[Y^{\alpha-1}(1-Y)^{\beta-1}\big]
\end{aligned}
$$
where $Y\sim U(0,x)$. Define the function as follows:(with B(3,3)=30)
```{r}
f1 <- function(x,n){
  u = runif(n,0,x)
  v = u^(2)*(1-u)^(2)
  return(x*mean(v)*30)
}
```
Then we have
```{r}
res = c()
for (i in 0.1*c(1:9)) {
  res <- append(res, f1(i,10000))
}
print(res)
```
The true value of cdf is
```{r}
pbeta(0.1*c(1:9),3,3)
```
### 5.9
First we have that 
$$
\begin{aligned}
F(x) &= \int_{-\infty}^x \frac{x}{\sigma^{2}} e^{-x^{2}/(2 \sigma^{2})}\mathbb{I}_{(0,\infty)}(x)dx\\
&=1-e^{-x^{2} /(2 \sigma^{2})}
\end{aligned}
$$
so that we obtain
$$
F^{-1}(u) = \sqrt{-2\sigma^2\ln(1-u)}.
$$
To apply antithetic method we use $1-u$ in exchange of $u$ to generate another group of samples
```{r}
Rayleigh_no_antithetic <- function(n,sigma){
  u = runif(n,0,1)
  x = sqrt(-2*sigma^2*log(1-u))
  return(x)
}
Rayleigh_with_antithetic <- function(n,sigma){
  u = runif(n,0,1)
  x1 = sqrt(-2*sigma^2*log(1-u))
  x2 = sqrt(-2*sigma^2*log(u))
  return((x1+x2)/2)
}
var_no_antithetic = var(Rayleigh_no_antithetic(10000,2)) 
var_with_antithetic = var(Rayleigh_with_antithetic(10000,2))
print(var_no_antithetic)
print(var_with_antithetic)
print(var_with_antithetic/var_no_antithetic)
```
Thus we have variance reduction equals to 1-0.02698519

### 5.13

First we have
$$
g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1
$$
We choose $X_1\sim N(0,1)$ with density $f_1(x)$, such that
$$
\begin{aligned}
\int_1^\infty g(x)dx &= \int_1^\infty x^2\cdot f_1(x)dx\\
&=E[X_1^2\mathbb{I}_{[1,\infty]}(X_1)]
\end{aligned}
$$
```{r}
x1 = rnorm(100000)
x1[x1<1] = 0
I1 = mean(x1^2)
print(I1)
print(var(x1^2)/100000)
```
estimator1 and its variance are shown above respectively.
 
Secondly we choose $X_2\sim Exp(1)+1$ with density $f_2(x)=e^{-x+1}\mathbb{I}_{[1,\infty]}$, such that
$$
\begin{aligned}
\int_1^\infty g(x)dx& =\int_{1}^\infty \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}dx\\
& = \int_{1}^\infty \frac{x^{2}}{\sqrt{2\pi}} e^{-x^{2}/2+x-1}f_2(x)dx\\
& = \frac{1}{\sqrt{2\pi}}E[X_2^2e^{-X_2^{2}/2(X_2-1)}]
\end{aligned}
$$
```{r}
x2 = rexp(100000,1) + 1
I2 = mean(x2^2*exp(-x2^2/2+x2-1)/sqrt(2*pi))
print(I2)
print(var(x2^2)/100000)
```
estimator2 and its variance are shown above respectively.

### 5.14
As is shown in 5.13, the estimated value is 0.4005021 by importance sampling.
 
# 21091-2021-10-14
 
## Questions
Exercises 6.5 and 6.A (page 180-181, Statistical Computating with R).

## Answers

### 6.5
Since $n=20$, t-statistic: $T=\sqrt{n}(\bar{X}-\mu)/s \sim t_{19}$, we have $(1-\alpha)$ confidence interval:$\big[\bar{X}-s\cdot t_{0.975}/\sqrt{n},\bar{X}+s\cdot t_{0.975}/\sqrt{n}\big]$. When $X\sim \chi^2(2)$, $EX=2$, we estimate the coverage probability as follows,
```{r}
set.seed(123)
n = 20
count = 0
m =10000
for(i in 1:m){
  x = rchisq(n,2)
  lower = mean(x)-qt(0.975,n-1) *sd(x)/sqrt(n)
  upper = mean(x)+qt(0.975,n-1) *sd(x)/sqrt(n)
  if(upper>2 && lower<2)
    count = count + 1
}
# cover probability
print(count/m)
```

As for CI for variance, we have $varX = 4$ and CI:$\big[0,(n-1)s^{2}/\chi_{\alpha}^{2}\big]$, such that
```{r}
set.seed(12)
n = 20
count2 = 0
m =10000
for(i in 1:m){
  x = rchisq(n,2)
  upp = (n-1)*var(x)/qchisq(0.05,n-1)
  if(4< upp)
    count2 = count2 +1
}
# cover probability
print(count2/m)
```
As shown above, 0.928>0.8609, which proves the better robustness of t-interval.

### 6.A
Set $n=50,\alpha= 0.05$. We have t-statistics:$T = \sqrt{n}(\bar{X}-\mu)/s$, rejection zone: $\{|T|>t_{0.975}\}$
case1: $X\sim \chi^2(1)$ with $\mu=EX=1$:
```{r}
n = 50
m = 10000
count1 = 0
for(i in 1:m){
  x = rchisq(n,1)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count1 = count1 + 1
}
print(count1/m)
```
case 2: $X\sim U(0，2)$ with $\mu=EX=1$:
```{r}
n = 50
m = 10000
count2 = 0
for(i in 1:m){
  x = runif(n,0,2)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count2 = count2 + 1
}
print(count2/m)
```
case 3: $X\sim Exp(1)$ with $\mu=EX=1$:
```{r}
n = 50
m = 10000
count3 = 0
for(i in 1:m){
  x = rexp(n,1)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count3 = count3 + 1
}
print(count3/m)
```

The epirical type one error of each case respectively equals to 0.0778, 0.0465 and 0.0662, which is a little close to significance level 0.05. Setting $n=100$ and repeat the simulations, we have
```{r}
set.seed(1234)
n = 100
m = 10000
count1 = 0
for(i in 1:m){
  x = rchisq(n,1)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count1 = count1 + 1
}
print(count1/m)
count2 = 0
for(i in 1:m){
  x = runif(n,0,2)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count2 = count2 + 1
}
print(count2/m)
count3 = 0
for(i in 1:m){
  x = rexp(n,1)
  t = sqrt(n)*(mean(x)-1)/sd(x)
  if(abs(t)>qt(0.975,n-1))
    count3 = count3 + 1
}
print(count3/m)
```
epirical type one error: 0.0633, 0.0485, 0.0578, closer to 0.05. It's easy to see that, t1e of uniform distribution deviates least and that of chisquare distribution deviates most.

### question3
If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at 0.05 level.
 
(i) What is the corresponding hypothesis test problem?
 
(ii) What test should we use? Z-test, two-sample t-test, paired-t
test or McNemar test? Why?
 
(iii) Please provide the least necessary information for hypothesis testing.
  
Answers:
 
(i) H0: the powers for two methods are identical. H1: the powers for two methods are different.
 
(ii) McNemar test. This test is applied to 2 × 2 contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal, i.e. $p_b=p_c$ in the following table:
 
|                 | Test 2 positive | Test 2 negative | Row total |
|:---------------:|:---------------:|:---------------:|:---------:|
| Test 1 positive |        a        |        b        |   a + b   |
| Test 1 negative |        c        |        d        |   c + d   |
|   Column total  |      a + c      |      b + d      |     N     |
 
which means to test whether $\text{power of test1}=p_a+p_b = p_a+p_b=\text{power of test2}$.

 
(iii) Record the detailed test decision (value of a,b,c,d in the above table) of each experiment of both methods, and follow the McNemar test procedure.
 
# 21091-2021-10-21 

## Questions
Exercises 6.C (pages 182, Statistical Computating with R).

## Answers

### 6.C

#### Example 6.8
```{r}
library(MASS)
```

The function calculating b1d:
```{r}
b1d <- function(x) {
  n = nrow(x)
  d = ncol(x)
  mu_hat = colSums(x)/n # MLE of mu
  #sigma_hat = 0
  #for(i in c(1:n)){
    #sigma_hat = sigma_hat + (x[i,]-mu_hat) %*% t(x[i,]-mu_hat)
  #}
  # sum(apply(x,1,function(y) t(y-mu_hat) %*% inv_sigma_hat %*% (y-mu_hat)))/n
  sigma_hat = cov(x)*(n-1)/n # MLE of cov
  inv_sigma_hat = solve(sigma_hat)
  
  # loop calculation of skewness 
  # for(j1 in c(1:n)){
  #   for(j2 in c(1:n)){
  #     b = b + (t(x[j1,]-mu_hat) %*% inv_sigma_hat %*% (x[j2,]-mu_hat))^3
  #   }
  # }
  # b = b/n^2
  # matrix calculation of skewness
  xc = sweep(x,2,mu_hat)
  b = sum((xc %*% inv_sigma_hat %*% t(xc))^3)/n^2
  return(b)
}

```

Case $d=2$:
```{r}
set.seed(12345)
n <-  c(20,30,50,100)
p.reject <- numeric(length(n))
m <- 1000
d <-  2
upper <- qchisq(0.975, df = d*(d+1)*(d+2)/6)
lower <- qchisq(0.025, df = d*(d+1)*(d+2)/6)
for (i in 1:length(n)) {
  sktests <- numeric(m) 
  for (j in 1:m) {
    x = mvrnorm(n[i],rep(0,d),diag(rep(1,d)))
    b = b1d(x)*n[i]/6
    sktests[j] <- as.integer(b > upper || b < lower ) }
  p.reject[i] <- mean(sktests) 
}
data.frame(n = n, estimate = p.reject)
```

#### Example 6.10
```{r}
alpha <- 0.1
n <- 30
m <- 500
epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
N <- length(epsilon)
power <- numeric(N)
```

```{r}
d = 2 
# two multivariate normal components 
sigma_1 = diag(rep(1,d))
sigma_10 = diag(rep(10,d))
set.seed(4321)
for (j in 1:N) { 
  e <- epsilon[j]
  sktests <- numeric(m)
  for (i in 1:m) {
    # in each replication, count_1 samples ~ sigma_1, count_10 samples ~ sigma_10
    count_1 = rbinom(1, n, 1-e)
    count_10 = n - count_1
    # in case epsilon = 1 or 0; count_1 or count_10 = 0 
    if(count_10==0){
      x = mvrnorm(count_1,rep(0,d),sigma_1)
    } else if(count_1 ==0){
      x =  mvrnorm(count_10,rep(0,d),sigma_10)
    } else{
      x_1 <- mvrnorm(count_1,rep(0,d),sigma_1)
      x_10 <- mvrnorm(count_10,rep(0,d),sigma_10)
      x = rbind(x_1,x_10)
    }
    b = b1d(x)*n/6
    sktests[i] <- as.integer(b > upper|| b<lower) 
    }
  power[j] <- mean(sktests)
}

```



The result is shown below:
```{r}

data.frame(epsilon = epsilon, estimate = power)
```

```{r}
plot(epsilon, power, type = "b", xlab = bquote(epsilon), ylim = c(0,1)) 
abline(h = .1, lty = 3)
se <- sqrt(power * (1-power) / m) #add standard errors 
lines(epsilon, power+se, lty = 3)
lines(epsilon, power-se, lty = 3)
```
 
# 21091-2021-10-28

## Questions
Exercises 7.7, 7.8, 7.9, and 7.B (pages 213, Statistical Computating with R).

## Answers

```{r}
library(bootstrap)
data(scor)
data = scor
```

### 7.7
```{r}
theta <- function(x){
  n = nrow(x)
  Cov = cov(x)*(n-1)/n
  Eigen_values = eigen(Cov)$values
  return(max(Eigen_values)/sum(Eigen_values)) 
}
```

```{r}
set.seed(413321)
theta_hat = theta(data)
cat("sample estimate is:",theta_hat)
```
```{r}
B = 10000
n = nrow(data)
theta_star = numeric(B)
for(i in 1:B){
  index = sample(1:n,n,replace = TRUE)
  x_star = data[index,]
  theta_star[i] = theta(x_star)
}
cat("the bias is",mean(theta_hat-theta_star),"\n")
cat("the sd is",sd(theta_hat-theta_star))
```

### 7.8
```{r}
theta_hat = theta(data)
n = nrow(data)
theta_jacknife = numeric(n)
for(i in 1:n){
  x_jacknife = data[-i,]
  theta_star[i] = theta(x_jacknife)
}
cat("the jacknife estimate of bias is",(n-1)*mean(theta_hat-theta_star),"\n")
cat("the jacknife estimate of sd is",sqrt((n-1)^2*var(theta_hat-theta_star)/n))
```
### 7.9
(1) percentile confidence intervals

```{r}
# we directly use the theta_star generated in ex 7.7
quan_upper = quantile(theta_star,0.975)
quan_lower = quantile(theta_star,0.025)
cat("the interval is [",quan_lower,",",quan_upper,"]")
```
 
(2) BCa confidence intervals
```{r}
boot.BCa <-
  function(x, th0, th, stat, conf = .95) {
    x <- as.matrix(x)
    n <- nrow(x) 
    N <- 1:n
    alpha <- (1 + c(-conf, conf))/2 
    zalpha <- qnorm(alpha)
    z0 <- qnorm(sum(th < th0) / length(th))
    th.jack <- numeric(n)
    for (i in 1:n) {
      J <- N[1:(n-1)]
      th.jack[i] <- stat(x[-i, ]) 
    }
    L <- mean(th.jack) - th.jack
    a <- sum(L^3)/(6 * sum(L^2)^1.5)
    adj.alpha <- pnorm(z0 + (z0+zalpha)/(1-a*(z0+zalpha))) 
    limits <- quantile(th, adj.alpha, type=6) 
    return(list("est"=th0, "BCa"=limits))
}
```

```{r}
boot.BCa(data,theta_hat,theta_star,theta,0.95)
```


### 7.B
```{r}
# first construct estimate of skewness 
skew_statistic <- function(x){
  x_bar <- mean(x)
  m3 <- mean((x-x_bar)^3)
  m2 <- mean((x-x_bar)^2)
  return(m3/m2^(3/2))
}
```



(1) normal distribution

```{r}
n = 20
m = 200
# standard normal bootstrap confidence interval
standard_interval = array(0,c(m,2))
colnames(standard_interval)<- c("lower_bound","upper_bound")

# basic bootstrap confidence interval
basic_interval = array(0,c(m,2))
colnames(basic_interval)<- c("lower_bound","upper_bound")

# percentile confidence interval
percentile_interval = array(0,c(m,2))
colnames(percentile_interval)<- c("lower_bound","upper_bound")

for(i in 1:m){
  x = rnorm(n)
  skewness_hat = skew_statistic(x)
  B = 500
  skewness_star = numeric(B)
  for(j in 1:B){
    index = sample(1:n,n,replace = TRUE)
    x_star = x[index]
    skewness_star[j] = skew_statistic(x_star)
  }
  standard_interval[i,1] = skewness_hat - qnorm(0.975)*sd(skewness_star)
  standard_interval[i,2] = skewness_hat + qnorm(0.975)*sd(skewness_star)
  basic_interval[i,1] = 2*skewness_hat - quantile(skewness_star,0.975)
  basic_interval[i,2] = 2*skewness_hat - quantile(skewness_star,0.025)
  percentile_interval[i,1] = quantile(skewness_star,0.025)
  percentile_interval[i,2] = quantile(skewness_star,0.975)
  
}
```

```{r}
# convergence rate of three kinds of interval
cat("convergence rates for normal distribution shown below:\n")
cat("standard:",1-mean((standard_interval[,1]>0 | standard_interval[,2]<0)),"\n")
cat("basic:",1-mean(basic_interval[,1]>0 | basic_interval[,2]<0),"\n")
cat("percentile:",1-mean(percentile_interval[,1]>0 | percentile_interval[,2]<0))
```

 

(2) Chisquare distribution $\chi^2(5)$ with skewness $\sqrt{(8/5)}$
```{r}
true_skewness = sqrt(8/5)
n = 20
m = 200
# standard normal bootstrap confidence interval
standard_interval = array(0,c(m,2))
colnames(standard_interval)<- c("lower_bound","upper_bound")

# basic bootstrap confidence interval
basic_interval = array(0,c(m,2))
colnames(basic_interval)<- c("lower_bound","upper_bound")

# percentile confidence interval
percentile_interval = array(0,c(m,2))
colnames(percentile_interval)<- c("lower_bound","upper_bound")

for(i in 1:m){
  x = rchisq(n,5)
  skewness_hat = skew_statistic(x)
  B = 500
  skewness_star = numeric(B)
  for(j in 1:B){
    index = sample(1:n,n,replace = TRUE)
    x_star = x[index]
    skewness_star[j] = skew_statistic(x_star)
  }
  standard_interval[i,1] = skewness_hat - qnorm(0.975)*sd(skewness_star)
  standard_interval[i,2] = skewness_hat + qnorm(0.975)*sd(skewness_star)
  basic_interval[i,1] = 2*skewness_hat - quantile(skewness_star,0.975)
  basic_interval[i,2] = 2*skewness_hat - quantile(skewness_star,0.025)
  percentile_interval[i,1] = quantile(skewness_star,0.025)
  percentile_interval[i,2] = quantile(skewness_star,0.975)
  
}
```

```{r}
cat("convergence rates for chisquare distribution shown below:\n")
cat("standard:",1-mean((standard_interval[,1]>true_skewness | standard_interval[,2]<true_skewness)),"\n")
cat("basic:",1-mean(basic_interval[,1]>true_skewness | basic_interval[,2]<true_skewness),"\n")
cat("percentile:",1-mean(percentile_interval[,1]>true_skewness | percentile_interval[,2]<true_skewness))
```
 
# 21091-2021-11-04


## Questions
Exercise 8.2 (page 242, Statistical Computating with R)

## Answers

### 8.2
```{r}

n <- 15
N <- 30
set.seed(23421)
x <- rnorm(n,0,1)
y <- rnorm(n,0,1)
z <- c(x,y)
R <- 1000
reps <- numeric(R)
t0 <- cor(x,y,method = "spearman")
for (i in 1:R) {
  k <- sample(1:N, size = n, replace = FALSE)
  x1 <- z[k]; y1 <- z[-k]
  reps[i] <- cor(x1,y1,method="spearman")
}
p1 <- mean(abs(c(t0, reps)) >= abs(t0))
cat("Permutation test p_values:",p1)
```

```{r}
t = cor.test(x,y,alternative = 'two.sided', method = "spearman")
p2<- t$p.value
cat("cor.test p_values:",p2)
```

### Additional question
Design experiments for evaluating the performance of the NN, energy, and ball methods in various situations.

#### Unequal variances and equal expectations

```{r}
library(boot)
library(RANN)
library(energy)
library(Ball)
```

```{r}
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1) # What is the first column?
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 <= n1); i2 <- sum(block2 > n1)
(i1 + i2) / (k * n)
}
```

```{r}
m <- 100; 
k<-3; 
p<-2; 
mu1 <- 0.5
mu2 <- 1.5
set.seed(145)

n1 <- n2 <- 15; R<-100; n <- n1+n2; N = c(n1,n2)
eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R,
  sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)}

p.values <- matrix(NA,m,3)
  for(i in 1:m){
  x <- matrix(rnorm(n1*p,mu1,1),ncol=p);
  y <- matrix(rnorm(n2*p,mu2,1),ncol=p);
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=R,seed=i*12345)$p.value
}
alpha <- 0.1;
pow <- colMeans(p.values<alpha)

```
```{r}
pow
```
 
#### Unequal variances and unequal expectations

```{r}

mu1 <- 0
mu2 <- 1
sigma1 <- 1
sigma2 <- 1.5
n1 <- n2 <- 15; n <- n1+n2; N = c(n1,n2)

p.values <- matrix(NA,m,3)
  for(i in 1:m){
  x <- matrix(rnorm(n1*p,mu1,sigma1),ncol=p);
  y <- matrix(rnorm(n2*p,mu2,sigma2),ncol=p);
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=R,seed=i*12345)$p.value
}
alpha <- 0.1;
pow <- colMeans(p.values<alpha)
print(pow)
```
 
#### t distribution with 1 df (heavy-tailed distribution) and bimodel distribution (mixture of two normal distributions)


```{r}

df <- 1
n1 <- n2 <- 15 
n <- n1+n2; N = c(n1,n2)

p.values <- matrix(NA,m,3)
  for(i in 1:m){
  x <- matrix(rt(n1*p,df),ncol=p);
  num <- rbinom(1,n2,0.5)
  y1 <- matrix(rnorm(num*p,0,1),ncol=p)
  y2 <- matrix(rnorm((n2-num)*p,0,3),ncol=p)
  y <- rbind(y1,y2)
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=R,seed=i*12345)$p.value
}
alpha <- 0.1;
pow <- colMeans(p.values<alpha)
print(pow)
```

 
#### Unbalanced samples (say, 1 case versus 10 controls)
```{r}

n1 <- 2
n2 <- 20
n <- n1+n2; N = c(n1,n2)

p.values <- matrix(NA,m,3)
  for(i in 1:m){
  x <- matrix(rnorm(n1*p,0,1),ncol=p)
  y <- matrix(rnorm(n2*p,0,1),ncol=p)
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=R,seed=i*12345)$p.value
}
alpha <- 0.1;
pow <- colMeans(p.values<alpha)
print(pow)
```
 
# 21091-2021-11-11
 
## Question
Exercies 9.3 and 9.8 (pages 277-278, Statistical Computating with R).
For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to Rˆ< 1.2.
## Answers

### 9.3
```{r}
# main part of the following code is copied from the slides of professor Zhang
set.seed(4313)
m <- 10000
sigma <- 10
mean <- 0
x <- numeric(m)
x[1] <- 0
k <- 0
u <- runif(m)

for (i in 2:m) {
    xt <- x[i-1]
    y <- rnorm(1,0,sigma)
    num <- dcauchy(y) * dnorm(xt,0,sigma)
    den <- dcauchy(xt) * dnorm(y, 0,sigma)
    if (u[i] <= num/den){
      x[i] <- y
    } else {
      x[i] <- xt
      k <- k+1     #y is rejected
    }
}

index <- 5500:6000
y1 <- x[index]
plot(index, y1, type="l", main="", ylab="x")
```
```{r}
b <- 3000
y <- x[b:m]
a <- c(1:9)/10
rbind(qcauchy(a),quantile(y,a))
```


### 9.8
condition distribution are respectively Binomial(n, x2) and Beta(x1 + a, n − x1 + b)
```{r}
# main part of the following code is copied from the slides of professor Zhang
set.seed(10311)
N <- 5000 
burn <- 1000 
X <- matrix(0, N, 2)
a <- 2
b <- 3
n <- 8
X[1, ] <- c(2, 0.5) #initialize
for (i in 2:N) {
  x2 <- X[i-1, 2]
  X[i, 1] <- rbinom(1,n,x2)
  x1 <- X[i, 1]
  X[i, 2] <- rbeta(1, x1+a, n-x1+b)
}
b <- burn + 1
x <- X[b:N, ]
index <- 3000:3500
y1 <- X[index,1]
y2 <- X[index,2]
plot(index, y1, type="l", main="the chain of x", ylab="x")
plot(index, y2, type="l", main="the chain of y", ylab="y")
```

### extra question

#### 9.3
```{r}
Gelman.Rubin <- function(psi) {

    psi <- as.matrix(psi)
    n <- ncol(psi)
    k <- nrow(psi)

    psi.means <- rowMeans(psi)     #row means
    B <- n * var(psi.means)        #between variance est.
    psi.w <- apply(psi, 1, "var")  #within variances
    W <- mean(psi.w)               #within est.
    v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
    r.hat <- v.hat / W             #G-R statistic
    return(r.hat)
    }
```

```{r}
cauchy.chain <- function(N,initial_value,Sigma=1) {
    m <- N
    sigma <- Sigma
    x <- numeric(m)
    x[1] <- initial_value
    k <- 0
    u <- runif(m)
    for (i in 2:m) {
        xt <- x[i-1]
        y <- rnorm(1,xt,sigma)
        num <- dcauchy(y) * dnorm(xt,y,sigma)
        den <- dcauchy(xt) * dnorm(y,xt,sigma)
        if (u[i] <= num/den){
          x[i] <- y
        } else {
          x[i] <- xt
          k <- k+1     #y is rejected
        }
    }
    return(x)
}
```
```{r}
set.seed(15)
k <- 5          #number of chains to generate
n <- 4000     #length of chains
b <- 1000       #burn-in length

sigma <- 3
X <- matrix(0, nrow=k, ncol=n)

x0 <- c(-3, -1.5, 0, 1.5, 3)
for (i in 1:k)
    X[i, ] <- cauchy.chain(n,x0[i],sigma)
```


```{r}

psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
    psi[i,] <- psi[i,] / (1:ncol(psi))
rhat <- rep(0, n)
for (j in (b+1):n)
    rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```
 
#### 9.8

For $y$:
```{r}
bivariate_chain <- function(n,y_initial_value=0.5,x_initial_value=2){
  N <- n
  burn <- 1000 
  X <- matrix(0, N, 2)
  a <- 2
  b <- 3
  n <- 10
  X[1, 1] <- x_initial_value
  X[1,2] <- y_initial_value
  for (i in 2:N) {
    x2 <- X[i-1, 2]
    X[i, 1] <- rbinom(1,n,x2)
    x1 <- X[i, 1]
    X[i, 2] <- rbeta(1, x1+a, n-x1+b)
  }
  return(X)
}

k <- 5          #number of chains to generate
n <- 4000     #length of chains
b <- 1000       #burn-in length

X <- matrix(0, nrow=k, ncol=n)

y_intial_value = c(0.1,0.3,0.5,0.7,0.9)
for (i in 1:k)
    X[i, ] <- bivariate_chain(n,y_initial_value =y_intial_value[i])[,2]

psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
    psi[i,] <- psi[i,] / (1:ncol(psi))
rhat <- rep(0, n)
for (j in (b+1):n)
    rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```
$r<1.2$, this means $y$ converges. 
 
For $x$:
```{r}

k <- 5          #number of chains to generate
n <- 4000     #length of chains
b <- 1000       #burn-in length
set.seed(42341)
X <- matrix(0, nrow=k, ncol=n)

x_initial_value = c(1,3,5,7,9)
for (i in 1:k)
    X[i, ] <- bivariate_chain(n,x_initial_value = x_initial_value[i])[,1]

psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
    psi[i,] <- psi[i,] / (1:ncol(psi))
rhat <- rep(0, n)
for (j in (b+1):n)
    rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```
this means $x$ converges.
 
# 21091-2021-11-18
 
## Question 

Exercises 11.3 and 11.5 (pages 353-354, Statistical Computing with R)
 
## Answers
 
### 11.3
#### (a)
```{r}
euclidean_norm <- function(x){
  return(sqrt(sum(x^2)))
  }

general_term <- function(k,a,d){
  temp <-  (2*k+2)*log(euclidean_norm(a)) + log(gamma((1+d)/2)) + log(gamma(k+3/2)) - log(factorial(k)) - k*log(2) -log(2*k+1) - log(2*k+2) - log(gamma(k+d/2+1))
  return(exp(temp) * (-1)^k)
}
```
```{r}
general_term(7,c(1,2),2)
```

#### (b)
```{r}
# calculate till 2n+1
S_n <- function(n,a){
  s <- 0
  d <- length(a)
  for(i in 0:n){
    s <- s + general_term(i,a,d)
  }
  return(s)
}
```
#### (c)
```{r}
a <- c(1,2)
d <- 2
print(S_n(160,a) - S_n(159,a))
cat(S_n(160,a))
```
 
### 11.5
 
#### 11.4 result
```{r}
s <- function(a,k){
  1 - pt(sqrt((a^2*k)/(k+1-a^2)),df=k)
}

f_d <- function(a,k){
  s(a,k) - s(a,k-1)
}

K <- c(4:25,100,500,1000)
n <- length(K)
res <- numeric(n)
for(i in 1:n){
  res[i] <- uniroot(f_d,k=K[i],lower=0.01,upper=sqrt(K[i])-0.01)$root
}
round(res,3)
```
#### calculate 11.5
```{r}
ck <- function(a,k) (a^2*k)/(k+1-a^2)
term <- function(a,k){
  integrate(dt,df=k,lower =0,upper = ck(a,k))$value
}
f_d2 <- function(a,k){
  term(a,k) - term(a,k-1) 
}
```

```{r}
# in order to prevent error, eachupper bound should be set in a correct range
lower_bound <- 0.1
upper_bound <- numeric(n)
for(i in 1:n){
  s <-  seq(2,20,by = 0.05)
  for(j in 1:length(s)){
    if(f_d2(s[j],K[i])>0) next
    upper_bound[i] = s[j]
    break
  }
}
```

```{r}
res2 <- numeric(n)
for(i in 1:n){
  res2[i] <- uniroot(f_d2,k=K[i],lower=0.5,upper=upper_bound[i])$root
}
round(res2,3)
```

this is not consistent with the result of exercise11.4, thus we find that the integrate method is not that reliable.

### extra question
Denote $x=(x_1,...,x_n)$ by the true value of $y=(y_1,...,y_n)$. By basic calculation:
$$
\begin{aligned}
Q(\lambda,\hat{\lambda}) &= E\left[\log(\lambda^n\cdot e^{-\lambda\cdot\sum_{i=1}^nx_i})\big| y,\hat{\lambda}\right]\\
&= n\log\lambda - \lambda\cdot\sum_{i=1}^nE[x_i|y_i,\hat{\lambda}]
\end{aligned}
$$
Since
$$
E(x_i|y_i,\hat{\lambda}) = \left\{
\begin{array}{l}
y_i,&   y_i<\tau\\
\tau +1/\hat{\lambda},& y_i =\tau
\end{array}\right.
$$
We have 
$$
\lambda^{(i+1)}=\frac{n}{m/\hat{\lambda}+\sum_{i=1}^n y_i } ,\quad \text{where m is the number of y_i that equals to } \tau
$$
code:
```{r}
y = c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
n = length(y)
m = n-sum(y<1)
tau = 1
lambda <- 0
lambda0 <- 1
k <- 0
while(abs(lambda-lambda0)>0.00001 && k<100){
  lambda <- lambda0
  lambda0 <- n/(m/lambda+sum(y))
  k = k+1
}
cat("EM lambda:",lambda0)
```

As for MLE, we have
$$
l(\lambda) = \lambda^7\cdot e^{-\lambda\cdot\sum_{i=1}^7y^{(i)}}\cdot e^{-3\lambda} = \lambda^7\cdot e^{-6.75\lambda}
$$
where $y^(i)$ is order statistic, this leads to 
```{r}
f<-function(x) {x^7*exp(-6.75*x)}
optimize(f,lower=0.2,upper=1.3,maximum = T)
```
this equals to the result of EM alog.
 
# 21091-2021-11-25

## Questions
1.Exercises 1 and 5 (page 204, Advanced R)

2.Excecises 1 and 7 (page 214, Advanced R)

## Answers
### Page204 Ex1
```{r}
trims <- c(0, 0.1, 0.2, 0.5)
x <- rcauchy(100)
lapply(trims, function(trim) mean(x, trim = trim))
```


```{r}
lapply(trims, mean, x = x)
```
这两行代码实现的是同一件事。第一种是把固定x的以trim为输入的mean函数用匿名函数形式作为lapply的fun；第二种是直接用mean，并把x作为lapply的参数传入。

### Page204 Ex5
#### for ex3
```{r}
data <- mtcars
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg~disp+wt, 
  mpg~I(1/disp)+wt)
rsq <- function(mod) summary(mod)$r.squared
```
```{r}
model <- lapply(formulas,lm,data=data)
r2 <- lapply(model, rsq)
print(r2)
```
#### for ex4
```{r}
bootstraps <- lapply(1:10, function(i) {
         rows <- sample(1:nrow(mtcars), rep = TRUE)
         mtcars[rows, ]})

model <- lapply(bootstraps,lm,formula=mpg ~ disp)
r2 <- lapply(model, rsq)
print(r2)
```

### Page214 Ex1
```{r}
set.seed(1341)
df <- data.frame(matrix(rnorm(15),5,3))
print(df)
```
```{r}
vapply(df,sd,FUN.VALUE=c(sd=0))
```
```{r}
X4 = c("a","b","c","d","e")
df <- cbind(df,X4)
print(df)
```
```{r}
# use vapply to find numeric columns
num_col_index <-  vapply(df,class,FUN.VALUE = c("a"))=="numeric"
vapply(df[,num_col_index],sd,FUN.VALUE=c(sd=0))
```
### Page214 Ex7
Implement mcsapply():
```{r}
library(parallel)
boot_df <- function(x) x[sample(nrow(x), rep = T), ]

rsquared <- function(mod) summary(mod)$r.square

boot_lm <- function(i) {
  dat <- boot_df(mtcars)
  rsquared(lm(mpg ~ wt + disp, data = dat))
}

```

```{r}
mcsapply<-function (X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE, mc.preschedule = TRUE,
 mc.set.seed = TRUE, mc.silent = FALSE, mc.cores = getOption("mc.cores", 2L), 
  mc.cleanup = TRUE, mc.allow.recursive = TRUE, affinity.list = NULL )
{
  answer <- mclapply(X = X, FUN = FUN, ...,mc.preschedule = mc.preschedule, 
mc.set.seed = mc.set.seed, mc.silent = mc.silent, mc.cores = mc.cores, 
  mc.cleanup = mc.cleanup, mc.allow.recursive = mc.allow.recursive, affinity.list = affinity.list)
  if (USE.NAMES && is.character(X) && is.null(names(answer))) 
    names(answer) <- X
  if (!isFALSE(simplify) && length(answer)) 
    simplify2array(answer, higher = (simplify == "array"))
  else answer
}
```

```{r}
n <- 5e3
system.time(sapply(1:n, boot_lm))

#system.time(mcsapply(1:n, boot_lm, mc.cores = 4))
```
 
可以实现mcsapply是也因为发现sapply实际上是封装了lapply。但是vapply代码不是这么实现的，所以我实现不了mcvapply。
 
# 21091-2021-12-02
 
## Questions

Write an Rcpp function for Exercise 9.8 (page 278, Statistical Computing with R).

Compare the corresponding generated random numbers with pure R language using the function “qqplot”.

Campare the computation time of the two functions with the function “microbenchmark”.

Comments your results.

## Answers
### (1)
```{r}
library(Rcpp)
library(microbenchmark)
```

```{r}
set.seed(10311)

rbivariate_chain <- function(n){
  N <- n
  burn <- 1000 
  X <- matrix(0, N, 2)
  a <- 2
  b <- 3
  n <- 10
  X[1,1] <- 2
  X[1,2] <- 0.5
  for (i in 2:N) {
    x1 <- X[i-1, 1]
    x2 <- X[i-1, 2]
    X[i, 1] <- rbinom(1,n,x2)
    X[i, 2] <- rbeta(1, x1+a, n-x1+b)
  }
  return(X)
}
```

### (2)
```{r}
N = 5000
rX = rbivariate_chain(N)
# cppX = cppbivariate_chain(N)
cppX = cppbivariate_chain(N)
```

```{r}
qqplot(rX[2000:5000,1],cppX[2000:5000,1],main="qqplot of X1")
qqplot(rX[2000:5000,2],cppX[2000:5000,2],main="qqplot of X2")
```

### (3)
```{r}

ts = microbenchmark(gibsR=rbivariate_chain(N), gibsC=cppbivariate_chain(N))
summary(ts)
```
### (4)
两者生成的样本大致符合同一分布，但是cpp的速度快于r非常多

